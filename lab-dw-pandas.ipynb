{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25d7736c-ba17-4aff-b6bb-66eba20fbf4e",
   "metadata": {},
   "source": [
    "# Lab | Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1973e9e-8be6-4039-b70e-d73ee0d94c99",
   "metadata": {},
   "source": [
    "In this lab, we will be working with the customer data from an insurance company, which can be found in the CSV file located at the following link: https://raw.githubusercontent.com/data-bootcamp-v4/data/main/file1.csv\n",
    "\n",
    "The data includes information such as customer ID, state, gender, education, income, and other variables that can be used to perform various analyses.\n",
    "\n",
    "Throughout the lab, we will be using the pandas library in Python to manipulate and analyze the data. Pandas is a powerful library that provides various data manipulation and analysis tools, including the ability to load and manipulate data from a variety of sources, including CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8045146f-f4f7-44d9-8cd9-130d6400c73a",
   "metadata": {},
   "source": [
    "### Data Description\n",
    "\n",
    "- Customer - Customer ID\n",
    "\n",
    "- ST - State where customers live\n",
    "\n",
    "- Gender - Gender of the customer\n",
    "\n",
    "- Education - Background education of customers \n",
    "\n",
    "- Customer Lifetime Value - Customer lifetime value(CLV) is the total revenue the client will derive from their entire relationship with a customer. In other words, is the predicted or calculated value of a customer over their entire duration as a policyholder with the insurance company. It is an estimation of the net profit that the insurance company expects to generate from a customer throughout their relationship with the company. Customer Lifetime Value takes into account factors such as the duration of the customer's policy, premium payments, claim history, renewal likelihood, and potential additional services or products the customer may purchase. It helps insurers assess the long-term profitability and value associated with retaining a particular customer.\n",
    "\n",
    "- Income - Customers income\n",
    "\n",
    "- Monthly Premium Auto - Amount of money the customer pays on a monthly basis as a premium for their auto insurance coverage. It represents the recurring cost that the insured person must pay to maintain their insurance policy and receive coverage for potential damages, accidents, or other covered events related to their vehicle.\n",
    "\n",
    "- Number of Open Complaints - Number of complaints the customer opened\n",
    "\n",
    "- Policy Type - There are three type of policies in car insurance (Corporate Auto, Personal Auto, and Special Auto)\n",
    "\n",
    "- Vehicle Class - Type of vehicle classes that customers have Two-Door Car, Four-Door Car SUV, Luxury SUV, Sports Car, and Luxury Car\n",
    "\n",
    "- Total Claim Amount - the sum of all claims made by the customer. It represents the total monetary value of all approved claims for incidents such as accidents, theft, vandalism, or other covered events.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a72419b-20fc-4905-817a-8c83abc59de6",
   "metadata": {},
   "source": [
    "External Resources: https://towardsdatascience.com/filtering-data-frames-in-pandas-b570b1f834b9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8ece17-e919-4e23-96c0-c7c59778436a",
   "metadata": {},
   "source": [
    "## Challenge 1: Understanding the data\n",
    "\n",
    "In this challenge, you will use pandas to explore a given dataset. Your task is to gain a deep understanding of the data by analyzing its characteristics, dimensions, and statistical properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91437bd5-59a6-49c0-8150-ef0e6e6eb253",
   "metadata": {},
   "source": [
    "- Identify the dimensions of the dataset by determining the number of rows and columns it contains.\n",
    "- Determine the data types of each column and evaluate whether they are appropriate for the nature of the variable. You should also provide suggestions for fixing any incorrect data types.\n",
    "- Identify the number of unique values for each column and determine which columns appear to be categorical. You should also describe the unique values of each categorical column and the range of values for numerical columns, and give your insights.\n",
    "- Compute summary statistics such as mean, median, mode, standard deviation, and quartiles to understand the central tendency and distribution of the data for numerical columns. You should also provide your conclusions based on these summary statistics.\n",
    "- Compute summary statistics for categorical columns and providing your conclusions based on these statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd4e8cd8-a6f6-486c-a5c4-1745b0c035f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing numpy and pandas\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fced637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the dimensions of the dataset by determining the number of rows and columns it contains.\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/data-bootcamp-v4/data/main/file1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77187b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca7befa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77286528",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb17288d",
   "metadata": {},
   "outputs": [],
   "source": [
    " #list(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87807b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74630232",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.columns) # Here just vieuwing it as a list/vertical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917de793",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c8fd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape # Reviewing here how many rows and columns the dataset has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2a7b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e0f9b0bf",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Determine the data types of each column and evaluate whether they are appropriate for the nature of the variable. You should also provide suggestions for fixing any incorrect data types.\n",
    "\n",
    "Good: \n",
    "ST for State = object, good \n",
    "GENDER = object, good, categorical  \n",
    "Education = object, good, categorical \n",
    "Income = float64, good numeric\n",
    "Monthly Premium Auto = float64, good, numeric\n",
    "Policy Type = object, categorial \n",
    "Vehicle Class = Object, good, categorical\n",
    "Total Claim Amount = float 64, good, numeric\n",
    "\n",
    "Possible issues: \n",
    "Customer = object, might cause an issue and should be float 64 or int64. Currently it's saved with %. \n",
    "\n",
    "Possibility on how to correct: df[\"Customer Lifetime Value\"] = df[\"Customer Lifetime Value\"].str.replace(',', '').astype(float)\n",
    "\n",
    "Number of Open Complaints = object, not good. It should be either float64 or int64.\n",
    "Possibility on how to correct: df[\"Number of Open Complaints\"] = pd.to_numeric(df[\"Number of Open Complaints\"], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96173a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes # Here I am reviewing all the data types per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d98949",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique() # Reviewing here the unique values per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034fb2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.GENDER.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c4a2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ST.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5667d2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Education.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c65aa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the number of unique values for each column and determine which columns appear to be categorical. You should also describe the unique values of each categorical column and the range of values for numerical columns, and give your insights.\n",
    "\n",
    "unique_counts = df.nunique() # Identify the number of unique values for each column\n",
    "print(\"\\nUnique value counts per column: \")\n",
    "print(unique_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a9536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_corrections = {\n",
    "    \"Femal\": \"Female\",\n",
    "    \"female\": \"Female\",\n",
    "    \"F\": \"Female\", \n",
    "    \"Male\": \"M\",\n",
    "    \"WA\": \"Washington\",\n",
    "    \"AZ\": \"Arizona\",\n",
    "    \"Cali\": \"California\",\n",
    "    \"Bachelors\": \"Bachelor\"    \n",
    "}\n",
    "# Applying corrrections to the relevant columns\n",
    "for col in[\"GENDER\", \"ST\", \"Education\"]: \n",
    "    df[col] = df[col].replace(category_corrections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b39d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine categorical and numerical colums\n",
    "categorical_columns = [col for col in df.columns if df[col].dtype == \"object\" or unique_counts[col] <15]\n",
    "numerical_columns = [col for col in df.columns if df[col].dtype in [\"float64\", \"int64\"] and col not in categorical_columns]\n",
    "print(\"\\nCategorical Columns:\", categorical_columns)\n",
    "print(\"\\nNumerical Columns: \", numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6a5d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.values #The unique values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a52914a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Describe unique values for categorical columns \n",
    "print(\"\\nUnique values for categorical columns: \")\n",
    "for col in categorical_columns: \n",
    "    print(f\"nColumn: {col}\")\n",
    "    print(df[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4309e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad04c1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7789aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get min-max range for numerical columns \n",
    "print(\"\\nRange of values for numerical columns: \")\n",
    "for col in numerical_columns:\n",
    "    print(f\"{col}: Min = {df[col].min()}, Max = {df[col].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319030cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discrete numerical values with fewer than 15 unique values often represent categories rather than continuous numbers.\n",
    "\n",
    "#Providing insights based on unique value distribution\n",
    "print(\"\\n--- Insights ---\")\n",
    "\n",
    "#Checking for high-cardinality categorical columns\n",
    "for col in categorical_columns: \n",
    "    if unique_counts[col] >10: #The threshold of 10 unique values is an arbitrary rule of thumb used to detect high-cardinality categorical columns.\n",
    "        print(f\"-Column '{col} has a high number of unique categories ({unique_counts[col]}). Consider encoding methods like target or frequency condition.\") \n",
    "\n",
    "#Checking for potential categorical columns stored as numbers \n",
    "for col in numerical_columns:\n",
    "    if unique_counts[col] <15: \n",
    "        print(f\"Column '{col}' is numerical but has only {unique_counts[col]} unique values. This might be categorical. \")\n",
    "\n",
    "#Checking for missing or suspicious values \n",
    "missing_values = df.isna().sum()\n",
    "for col in df.columns: \n",
    "    if missing_values[col] >0: \n",
    "        print(f\"- Column '{col}' has {missing_values[col]} missing values. Consider handling them appropriatly.\")\n",
    "\n",
    "print(\"\\nThis was the analysis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4109c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute summary statistics such as mean, median, mode, standard deviation, and quartiles to understand the central tendency and distribution of the data for numerical columns. You should also provide your conclusions based on these summary statistics.\n",
    "\n",
    "print(\"\\n--- Summary ---\")\n",
    "\n",
    "#numerical_columns = [col for col in df.columns if df[col].dtype in [\"int64\", \"float\"]], I already have defined all the numerical columns with this code. \n",
    "\n",
    "summary = df[numerical_columns].agg([\"mean\", \"median\", lambda x: x.mode()[0]])\n",
    "summary.rename(index={summary.index[2]: \"mode\"}, inplace=True)\n",
    "print(summary)\n",
    "\n",
    "print(\"\\nEnd of Summary!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ec4fc4",
   "metadata": {},
   "source": [
    "Conclusions Based on the Summary Statistics:\n",
    "\n",
    "1. Income Analysis: \n",
    "Mean: 39295\n",
    "Median: 36234\n",
    "Mode: 0.00\n",
    "\n",
    "Insights: \n",
    "- The mean income is higher than the median. However, the most frequent income is 0.00. So there are customers who have very high incomes, pulling the average up. \n",
    "- The mode is 0.00, which is unusual and can indicate the following: \n",
    "    - Missing or incorrect values encoded as 0. \n",
    "    - Many customers with no recorded income. \n",
    "    - The dataset includes unemployed individuals or students. \n",
    "\n",
    "2. Monthly Premium Auto (Car Insurance Premiums)\n",
    "Mean: 193.23\n",
    "Median: 83.00\n",
    "Mode: 65.00\n",
    "\n",
    "Insights: \n",
    "- The mean is significantly  higher tham the median. \n",
    "- Most common premium is 65.00, which means customers pay a lower amount, but a few very high premiums.\n",
    "    - A few customers have very expensive insurance plans. \n",
    "    - Some premium values might be outliers. \n",
    "\n",
    "3. Total Claim Amount (Payouts for Insurance Claims)\n",
    "Mean: 404.99\n",
    "Median: 354.73\n",
    "Mode: 321.60\n",
    "\n",
    "Insights: \n",
    "- The mean is slightly higher than the median.\n",
    "- Most frequent claim is 321.60, which suggest that small claims are common. \n",
    "- The median and mode are close, which suggest a fair balanced distribution. \n",
    "\n",
    "Next steps: \n",
    "- Identify and check for outliers.\n",
    "- Investigate how many customers have a 0.00 income.\n",
    "- Consider log tranformation if the skewness is too high. \n",
    "- Investigate potetial high-value claims to see if they are valid or outliers.\n",
    "- Plot a histogram to check the claim distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5745637",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Investigate how many customers have a 0.00 income.\n",
    "print(\"Amount of missing income values:\", df[\"Income\"].value_counts()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c46532",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify and check for outliers.\n",
    "df[\"Monthly Premium Auto\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23570763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram to check the claim distribution\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "df[\"Total Claim Amount\"].hist(bins=30)\n",
    "plt.xlabel(\"Total Claim Amount\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Total Claim Amount\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e8fe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another way to do the summary: \n",
    "\n",
    "mean_values = df[numerical_columns].mean()\n",
    "print(\"\\nMean:\\n\", mean_values)\n",
    "\n",
    "median_values = df[numerical_columns].median()\n",
    "print(\"\\nMedian:\\n\", median_values)\n",
    "\n",
    "mode_values = df[numerical_columns].mode().iloc[0]\n",
    "print(\"\\nMode:\\n\", mode_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685dacf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute summary statistics for categorical columns and providing your conclusions based on these statistics.\n",
    "\n",
    "print(\"\\n--- Categorical Summary ---\")\n",
    "\n",
    "# Create a summary DataFrame\n",
    "categorical_summary = pd.DataFrame({\n",
    "    \"Unique Categories\": [df[col].nunique() for col in categorical_columns],\n",
    "    \"Most Frequent Category\": [df[col].mode()[0] for col in categorical_columns],\n",
    "    \"Most Frequent Count\": [df[col].value_counts().iloc[0] for col in categorical_columns]\n",
    "}, index=categorical_columns)\n",
    "\n",
    "print(categorical_summary)\n",
    "\n",
    "#To get a more detailed breakdown, looping through each categorical column:\n",
    "for col in categorical_columns: \n",
    "    print(f\"\\nColumn: {col}\")\n",
    "    print(df[col].value_counts(normalize=True) *100)\n",
    "\n",
    "print(\"\\nEnd of Categorical Summary!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a703890-63db-4944-b7ab-95a4f8185120",
   "metadata": {},
   "source": [
    "## Challenge 2: analyzing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0776a403-c56a-452f-ac33-5fd4fdb06fc7",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedbc484-da4d-4f9c-9343-e1d44311a87e",
   "metadata": {},
   "source": [
    "The marketing team wants to know the top 5 less common customer locations. Create a pandas Series object that contains the customer locations and their frequencies, and then retrieve the top 5 less common locations in ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dca5073-4520-4f42-9390-4b92733284ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location Counts\n",
    "\n",
    "location_counts = df[\"ST\"].value_counts()\n",
    "\n",
    "leads_common_locations = location_counts.nsmallest(5)\n",
    "\n",
    "print(leads_common_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce80f43-4afa-43c7-a78a-c917444da4e0",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "The sales team wants to know the total number of policies sold for each type of policy. Create a pandas Series object that contains the policy types and their total number of policies sold, and then retrieve the policy type with the highest number of policies sold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f13997-1555-4f98-aca6-970fda1d2c3f",
   "metadata": {},
   "source": [
    "*Hint:*\n",
    "- *Using value_counts() method simplifies this analysis.*\n",
    "- *Futhermore, there is a method that returns the index of the maximum value in a column or row.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfad6c1-9af2-4b0b-9aa9-0dc5c17473c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "policy_counts = df[\"Policy Type\"].value_counts()\n",
    "top_policy_type = policy_counts.idxmax() #Gets the policy type with the highest count\n",
    "top_policy_count = policy_counts.max() #Gets the highest count\n",
    "\n",
    "print(\"Total policies sold per policy type:\\n\", \n",
    "      policy_counts)\n",
    "print(f\"\\nThe Policy type with the highest number of policies sold is '{top_policy_type}' with {top_policy_count} policies in total.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b863fd3-bf91-4d5d-86eb-be29ed9f5b70",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "The sales team wants to know if customers with Personal Auto have a lower income than those with Corporate Auto. How does the average income compare between the two policy types?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1386d75-2810-4aa1-93e0-9485aa12d552",
   "metadata": {},
   "source": [
    "- Use *loc* to create two dataframes: one containing only Personal Auto policies and one containing only Corporate Auto policies.\n",
    "- Calculate the average income for each policy.\n",
    "- Print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0563cf-6f8b-463d-a321-651a972f82e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "personal_auto_df = df.loc[df[\"Policy Type\"] == 'Personal Auto']\n",
    "corporate_auto_df = df.loc[df['Policy Type'] == 'Corporate Auto']\n",
    "\n",
    "avg_income_personal_auto = personal_auto_df['Income'].mean()\n",
    "avg_income_corporate_auto = corporate_auto_df['Income'].mean()\n",
    "\n",
    "print(f\"Average Income for Personal Auto policies: ${avg_income_personal_auto:.2f}\")\n",
    "print(f\"Average Income for Corporate Auto policies: ${avg_income_corporate_auto:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b16c27-f4a5-4727-a229-1f88671cf4e2",
   "metadata": {},
   "source": [
    "### Bonus: Exercise 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac584986-299b-475f-ac2e-928c16c3f512",
   "metadata": {},
   "source": [
    "Your goal is to identify customers with a high policy claim amount.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "- Review again the statistics for total claim amount to gain an understanding of the data.\n",
    "- To identify potential areas for improving customer retention and profitability, we want to focus on customers with a high policy claim amount. Consider customers with a high policy claim amount to be those in the top 25% of the total claim amount. Create a pandas DataFrame object that contains information about customers with a policy claim amount greater than the 75th percentile.\n",
    "- Use DataFrame methods to calculate summary statistics about the high policy claim amount data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d234634-50bd-41e0-88f7-d5ba684455d1",
   "metadata": {},
   "source": [
    "*Hint 2: check `Boolean selection according to the values of a single column` in https://towardsdatascience.com/filtering-data-frames-in-pandas-b570b1f834b9*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3af5f1-6023-4b05-9c01-d05392daa650",
   "metadata": {},
   "source": [
    "*Note: When analyzing data, we often want to focus on certain groups of values to gain insights. Percentiles are a useful tool to help us define these groups. A percentile is a measure that tells us what percentage of values in a dataset are below a certain value. For example, the 75th percentile represents the value below which 75% of the data falls. Similarly, the 25th percentile represents the value below which 25% of the data falls. When we talk about the top 25%, we are referring to the values that fall above the 75th percentile, which represent the top quarter of the data. On the other hand, when we talk about the bottom 25%, we are referring to the values that fall below the 25th percentile, which represent the bottom quarter of the data. By focusing on these groups, we can identify patterns and trends that may be useful for making decisions and taking action.*\n",
    "\n",
    "*Hint: look for a method that gives you the percentile or quantile 0.75 and 0.25 for a Pandas Series.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b731bca6-a760-4860-a27b-a33efa712ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "summary_stats = df['Total Claim Amount'].describe()\n",
    "print(\"Summary Statistics:\\n\", summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7bf6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_75 = df[\"Total Claim Amount\"].quantile(0.75)\n",
    "percentile_75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ee2cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_claim_customers = df[df['Total Claim Amount'] > percentile_75]\n",
    "high_claim_summary = high_claim_customers.describe()\n",
    "\n",
    "from IPython.display import display\n",
    "display(high_claim_customers)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
